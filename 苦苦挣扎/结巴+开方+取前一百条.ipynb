{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = pd.read_excel('数据比较好的.xlsx')\n",
    "Y = txt['标签']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\hebo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.798 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(Y)\n",
    "stopwords = [line.strip() for line in open('../input/stopword.txt', 'r', encoding='utf-8').readlines()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_des = txt['商品描述'].tolist()\n",
    "X = []\n",
    "with open('分词拿去做word2vect.txtct.txt','w',encoding = 'utf8') as file:\n",
    "    for i in range(len(list_des)):\n",
    "        des = list_des[i].replace(' ','')\n",
    "        seg_list = jieba.lcut(des)\n",
    "        word_list = [] \n",
    "        for seg in seg_list:\n",
    "            if seg not in stopwords:\n",
    "                word_list.append(seg)\n",
    "        X.append((' ').join(word_list))\n",
    "        file.write((' ').join(word_list) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt['商品描述'] = pd.Series(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(columns=['商品描述','标签'])\n",
    "dic = Counter(Y)\n",
    "for i in dic.keys():\n",
    "    data = txt.loc[txt['标签'] == i]\n",
    "    if dic[i] > 200:\n",
    "        frame = pd.concat([frame,data[:200]],axis = 0)\n",
    "        continue\n",
    "    else:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61467, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = frame.reset_index(drop=True)\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "fre_matrix = vectorizer.fit_transform(frame['商品描述'])\n",
    "feature_names  = vectorizer.get_feature_names()\n",
    "word_index_dic = vectorizer.vocabulary_\n",
    "length = (fre_matrix.shape)[1]\n",
    "newd = {v:k for k,v in word_index_dic.items()}\n",
    "bag_words = []\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf = tfidf.fit_transform(fre_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for i in range(61497):\n",
    "    print('----Document %d----' % (i))\n",
    "    for j in range(length):\n",
    "        if tfidf[i,j] > 1e-5:\n",
    "              print( feature_names[j], tfidf[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for label in Counter(frame['标签']).keys():\n",
    "    if n % 100 == 0:\n",
    "        print(n)\n",
    "    choose = frame.loc[frame['标签'] == label].index\n",
    "    A = fre_matrix[choose].sum(axis=0)\n",
    "    C = np.zeros((1,length),np.int64)\n",
    "    index_list = [] \n",
    "    for i in range(length):\n",
    "        if A[0,i] != 0:\n",
    "            index_list.append(i)\n",
    "            C[0,i] = len(choose) - A[0,i]\n",
    "    choose_1 = frame.loc[frame['标签'] != label].index\n",
    "    B_ = np.zeros((1,length),np.int64)\n",
    "    for index in choose_1:\n",
    "        B_ = fre_matrix[index] + B_\n",
    "    B = np.zeros((1,length),np.int64)\n",
    "    for index in index_list:\n",
    "        B[0,index] = B_[0,index]\n",
    "    D = np.zeros((1,length),np.int64)\n",
    "    for i in range(length):\n",
    "        if B[0,i] != 0:\n",
    "            D[0,i] = len(choose_1) - B[0,i]\n",
    "    AD_BC_2 = (np.array(A)*np.array(D) - np.array(B)*np.array(C))**2\n",
    "    chi2 =  AD_BC_2  / ((np.array(A) + np.array(B))*(np.array(C)+ np.array(D)))\n",
    "    hope_index = {}\n",
    "    for i in range(length):\n",
    "        if isnan(chi2[0,i]) == False:\n",
    "            hope_index[i] = chi2[0,i]\n",
    "    li = sorted(hope_index.items(),key = lambda x:x[1],reverse = True)\n",
    "    #for i in range(2):\n",
    "        #bag_words.append(newd[li[i][0]])\n",
    "    bag_words.append(newd[li[0][0]])\n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2262 2239\n"
     ]
    }
   ],
   "source": [
    "good_words = list(set(bag_words))\n",
    "print(len(bag_words),len(set(bag_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chi2(matrix,weight):\n",
    "    matrix_1 = copy.deepcopy(matrix)\n",
    "    for row in range(matrix_1.shape[0]):\n",
    "        for word in frame['商品描述'][row].split():\n",
    "            if word in good_words:\n",
    "                col = word_index_dic[word]\n",
    "                matrix_1[row,col] += weight\n",
    "    return matrix_1\n",
    "def LR(X,Y):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    classifier = LogisticRegression(random_state = 0)\n",
    "    classifier.fit(X_train, Y_train)# coding:utf-8\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(np.mean(y_pred == Y_test))\n",
    "    return (np.mean(y_pred == Y_test))\n",
    "def RF(X,Y):\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "    classifier = RandomForestClassifier(random_state = 0)\n",
    "    classifier.fit(X_train, Y_train)# coding:utf-8\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(np.mean(y_pred == Y_test))\n",
    "    return (np.mean(y_pred == Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词频加开方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7601268911664226\n"
     ]
    }
   ],
   "source": [
    "X_matrix = _chi2(fre_matrix,1)\n",
    "accuracy = LR(X_matrix,frame['标签'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单纯的词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7369448511469009\n"
     ]
    }
   ],
   "source": [
    "accuracy = LR(fre_matrix,frame['标签'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单纯的tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6394989425736132\n"
     ]
    }
   ],
   "source": [
    "accuracy = LR(tfidf,frame['标签'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf+开方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7545143972669595\n"
     ]
    }
   ],
   "source": [
    "X_matrix = _chi2(tfidf,1)\n",
    "accuracy = LR(X_matrix,frame['标签'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf+开方+word2vec每一个维度最大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "sentences = word2vec.Text8Corpus('分词拿去做word2vect.txt')\n",
    "model = word2vec.Word2Vec(sentences,size = 100,window = 900 ,min_count = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_max = []\n",
    "Y_max = []\n",
    "for i in range(len(frame['商品描述'])):\n",
    "    matrix = []\n",
    "    misnum = 0\n",
    "    for word in frame['商品描述'][i].split():\n",
    "        try:\n",
    "            matrix.append(model[word].tolist())\n",
    "        except:\n",
    "            misnum += 1\n",
    "    max_list = []\n",
    "    for j in range(len(matrix[0])):  \n",
    "        one_list=[]  \n",
    "        for k in range(len(matrix)):  \n",
    "            one_list.append(matrix[k][j])  \n",
    "        max_list.append(max(one_list))\n",
    "    X_max.append(max_list)\n",
    "tfidf_chi2 = _chi2(tfidf,1.5)\n",
    "from scipy.sparse import hstack,vstack,csc_matrix\n",
    "#word2vec_matrix = csc_matrix(X_max) 可以将X_max转化为稀疏矩阵\n",
    "tfidf_chi2_word2vec = hstack((tfidf_chi2,X_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7425573450463641\n"
     ]
    }
   ],
   "source": [
    "accuracy = LR(tfidf_chi2_word2vec,frame['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7586627623230844\n"
     ]
    }
   ],
   "source": [
    "accuracy = LR(tfidf_chi2,frame['标签'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf+开方+word2vec每一个维度均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [des.split() for des in frame['商品描述']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3058934, 3241835)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec \n",
    "model = Word2Vec(size = 100 ,window = 500 , min_count = 3)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "word2vac_avg = []\n",
    "for line in range(len(frame['商品描述'])):\n",
    "    su = np.zeros((1,100))\n",
    "    words = frame['商品描述'][line].split()\n",
    "    num = 1\n",
    "    for word in words:\n",
    "        try:\n",
    "            temp = model[word]\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            temp = temp.reshape((1,100))\n",
    "            su += temp\n",
    "            num = num+1\n",
    "    if num > 1:\n",
    "        num -= 1\n",
    "    su /= num\n",
    "    word2vac_avg.append(su)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svc = []\n",
    "for i in range(len(word2vac_avg)):\n",
    "    X_svc.append(list(word2vac_avg[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24548560273304051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_svc,frame['标签'],test_size = 0.2)\n",
    "clf.fit(X_train,Y_train)\n",
    "result = clf.predict(X_test)\n",
    "print(np.mean(result == Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_chi2 = _chi2(tfidf,1.5)\n",
    "tfidf_chi2_word2vec_avg = hstack((tfidf_chi2,np.array(word2vac_avg).reshape(-1,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755165121197332\n"
     ]
    }
   ],
   "source": [
    "accuracy = LR(tfidf_chi2_word2vec_avg,frame['标签'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在试一下啦啦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 3)\n",
    "fre_matrix = vectorizer.fit_transform(frame['商品描述'])\n",
    "feature_names  = vectorizer.get_feature_names()\n",
    "word_index_dic = vectorizer.vocabulary_\n",
    "length = (fre_matrix.shape)[1]\n",
    "newd = {v:k for k,v in word_index_dic.items()}\n",
    "bag_words = []\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf = tfidf.fit_transform(fre_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6429965836993655\n"
     ]
    }
   ],
   "source": [
    "#X_matrix = min_df_chi2(tfidf,1)\n",
    "accuracy = LR(tfidf,frame['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_df_chi2(matrix,weight):\n",
    "    matrix_1 = copy.deepcopy(matrix)\n",
    "    for row in range(fre_matrix.shape[0]):\n",
    "        for word in frame['商品描述'][row].split():\n",
    "            if word in good_words:\n",
    "                try:\n",
    "                    col = word_index_dic[word]\n",
    "                    matrix_1[row,col] += weight\n",
    "                except:\n",
    "                    a = 1\n",
    "    return matrix_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61467, 14379)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'岩田'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-9f1964d438b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrf_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_chi2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfre_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mRF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'标签'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-128968c447cb>\u001b[0m in \u001b[0;36m_chi2\u001b[1;34m(matrix, weight)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'商品描述'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgood_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                 \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_index_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m                 \u001b[0mmatrix_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatrix_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '岩田'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_x = _chi2(fre_matrix,1)\n",
    "RF(rf_x,frame['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR(rf_x,frame['标签'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字加词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack,vstack,csc_matrix\n",
    "\n",
    "vectorizer_char = CountVectorizer(analyzer='char',min_df = 4)\n",
    "fre_matrix_char = vectorizer_char.fit_transform(frame['商品描述']) # 2783\n",
    "\n",
    "vectorizer_fre = CountVectorizer(min_df = 4)\n",
    "fre_matrix_ = vectorizer_fre.fit_transform(frame['商品描述']) #11965\n",
    "\n",
    "fre_char_matrix = hstack([fre_matrix_char,fre_matrix_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_1 = LR(fre_matrix_char,frame['标签']) # 按字做\n",
    "acc_2 = LR(fre_matrix_,frame['标签'])  # 词频\n",
    "acc_3 = LR(fre_char_matrix,frame['标签'])  #词频加字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_dic_ = vectorizer_fre.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_1 = copy.deepcopy(fre_matrix_)\n",
    "for row in range(fre_matrix_.shape[0]):\n",
    "    for word in frame['商品描述'][row].split():\n",
    "        if word in good_words and word in word_index_dic_.keys():\n",
    "            col = word_index_dic_[word]\n",
    "            matrix_1[row,col] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_char_matrix = hstack([fre_matrix_char,matrix_1])\n",
    "acc_4 = LR(fre_char_matrix,frame['标签']) #词加开方 + 字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR(matrix_1,frame['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
