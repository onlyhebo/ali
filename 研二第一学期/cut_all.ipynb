{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\njieba.load_userdict(Y)\\nstopwords = [line.strip() for line in open('../input/stopword.txt', 'r', encoding='utf-8').readlines()] \\nlist_des = txt['商品描述'].tolist()\\nX = []\\nfor i in range(len(list_des)):\\n    des = list_des[i].replace(' ','')\\n    seg_list = list(jieba.lcut(des))\\n    word_list = [] \\n    for seg in seg_list:\\n        if seg not in stopwords:\\n            word_list.append(seg)\\n    X.append((' ').join(word_list))\\ntxt['商品描述'] = pd.Series(X)  \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#txt = pd.read_excel('数据比较好的.xlsx')\n",
    "txt = pd.read_excel('guo(new_3w).xlsx').drop('空白字段',axis = 1)\n",
    "Y = txt['标签']\n",
    "'''\n",
    "jieba.load_userdict(Y)\n",
    "stopwords = [line.strip() for line in open('../input/stopword.txt', 'r', encoding='utf-8').readlines()] \n",
    "list_des = txt['商品描述'].tolist()\n",
    "X = []\n",
    "for i in range(len(list_des)):\n",
    "    des = list_des[i].replace(' ','')\n",
    "    seg_list = list(jieba.lcut(des))\n",
    "    word_list = [] \n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:\n",
    "            word_list.append(seg)\n",
    "    X.append((' ').join(word_list))\n",
    "txt['商品描述'] = pd.Series(X)  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt)):\n",
    "    length = len(txt.loc[i]['商品描述'].split())\n",
    "    if length >15:\n",
    "        txt.drop(index = i,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量：31674 分类数目：511\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame(columns=['商品描述','标签'])\n",
    "dic = Counter(Y)\n",
    "for i in dic.keys():\n",
    "    data = txt.loc[txt['标签'] == i]\n",
    "    '''if dic[i] > 200:\n",
    "        frame = pd.concat([frame,data[:200]],axis = 0)\n",
    "        continue\n",
    "    elif dic[i]>=10:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)'''\n",
    "    if dic[i]>=10:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)\n",
    "frame = frame.reset_index(drop=True)\n",
    "c_num = len(Counter(frame['标签']).keys())\n",
    "print('数据量：%d'%frame.shape[0],'分类数目：%d'%len(Counter(frame['标签']).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "word_txt = []\n",
    "for label in Counter(frame['标签']).keys():\n",
    "    same = frame.loc[frame['标签'] == label]\n",
    "    word = []\n",
    "    for i in same['商品描述']:\n",
    "        word += i.split()\n",
    "    word_txt.append(word)\n",
    "from gensim.models import word2vec\n",
    "#sentences = word2vec.Text8Corpus('分词拿去做word2vect.txt')\n",
    "model = word2vec.Word2Vec(word_txt,size = 200,window = 2000,min_count = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "token_dictionary = {}\n",
    "token_index = 0\n",
    "word2vec_matrix = []\n",
    "for line in range(len(frame['商品描述'])):\n",
    "    tokens = frame['商品描述'][line].split()\n",
    "    del_list = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            temp = model[token]\n",
    "        except:\n",
    "            del_list.append(token)\n",
    "            continue\n",
    "        else:\n",
    "            if token not in token_dictionary:\n",
    "                word2vec_matrix.append(list(temp))\n",
    "                token_dictionary[token] = token_index\n",
    "                token_index += 1\n",
    "    for token in del_list:\n",
    "        tokens.remove(token)\n",
    "    frame['商品描述'][line] = ' '.join(tokens)\n",
    "word2vec_matrix.append([0]*200)\n",
    "token_dictionary['</s>'] = token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_index = pd.DataFrame(columns=['商品描述','标签'])\n",
    "padding_index = len(token_dictionary) - 1\n",
    "index = []\n",
    "for line in frame['商品描述']:\n",
    "    tokens = line.split()\n",
    "    line_index = []\n",
    "    for token in tokens:\n",
    "        line_index += [token_dictionary[token]]\n",
    "    for _ in range(len(line_index),15):\n",
    "        line_index.append(padding_index)\n",
    "    index.append(line_index)\n",
    "    \n",
    "frame_index['商品描述'] = pd.Series(index)\n",
    "frame_index['标签'] = frame['标签']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(frame_index['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "input_size = 200\n",
    "timestep_size = 15\n",
    "hidden_size = 256\n",
    "layer_num = 2\n",
    "class_num = c_num\n",
    "learning_rate = 0.01\n",
    "embed_dim = 200\n",
    "sequence_length = 15\n",
    "with tf.name_scope('inputs'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    batch_size = tf.placeholder(tf.int32, [], name='batch_size_input')\n",
    "    #_X = tf.placeholder(tf.float32,[None,1500])\n",
    "    y = tf.placeholder(tf.int64,[None])\n",
    "    #x = tf.reshape(_X,[-1,timestep_size,input_size])\n",
    "    input_data = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_data') \n",
    "with tf.name_scope(\"embedding_layer\"):\n",
    "    #embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\n",
    "    embedding = tf.Variable(word2vec_matrix,dtype = tf.float32)\n",
    "    inputs=tf.nn.embedding_lookup(embedding,input_data)    \n",
    "    \n",
    "#inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "#input_data (batch_size , sequence_length)\n",
    "#inputs shape : (batch_size , sequence_length , embed_dim)\n",
    "\n",
    "inputs = tf.transpose(inputs, [1,0,2])\n",
    "#inputs shape : (sequence_length, batch_size, embed_dim)\n",
    "\n",
    "inputs = tf.reshape(inputs, [-1, embed_dim])\n",
    "\n",
    "# 转换成list,里面的每个元素是(batch_size, embed_dim)\n",
    "inputs = tf.split(inputs, sequence_length, 0)\n",
    "\n",
    "with tf.name_scope(\"fw\"):\n",
    "    stacked_rnn_fw = []\n",
    "    for _ in range(layer_num):\n",
    "        #fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        fw_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "        stacked_rnn_fw.append(fw_cell)\n",
    "    lstm_fw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_fw), output_keep_prob= keep_prob)\n",
    "    #lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "with tf.name_scope(\"bw\"):\n",
    "    stacked_rnn_bw = []\n",
    "    for _ in range(layer_num):\n",
    "        #bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        bw_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "        stacked_rnn_bw.append(bw_cell)\n",
    "    lstm_bw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_bw), output_keep_prob= keep_prob)\n",
    "    #lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "with tf.name_scope(\"output\"):\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, inputs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_size = 256\n",
    "with tf.name_scope('attention'), tf.variable_scope('attention'):\n",
    "    attention_w = tf.Variable(tf.truncated_normal([2*hidden_size, attention_size], stddev=0.1), name='attention_w')\n",
    "    attention_b = tf.Variable(tf.constant(0.1, shape=[attention_size]), name='attention_b')\n",
    "    u_list = []\n",
    "    for t in range(15):\n",
    "        u_t = tf.tanh(tf.matmul(outputs[t], attention_w) + attention_b) \n",
    "        u_list.append(u_t)\n",
    "    u_w = tf.Variable(tf.truncated_normal([attention_size, 1], stddev=0.1), name='attention_uw')\n",
    "    attn_z = []\n",
    "    for t in range(15):\n",
    "        z_t = tf.matmul(u_list[t], u_w)\n",
    "        attn_z.append(z_t)\n",
    "    # transform to batch_size * sequence_length\n",
    "    attn_zconcat = tf.concat(attn_z, axis=1)\n",
    "    alpha = tf.nn.softmax(attn_zconcat)\n",
    "    # transform to sequence_length * batch_size * 1 , same rank as outputs\n",
    "    alpha_trans = tf.reshape(tf.transpose(alpha, [1,0]), [15, -1, 1])\n",
    "    final_output = tf.reduce_sum(outputs * alpha_trans, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"result\",reuse = tf.AUTO_REUSE):\n",
    "    fc_w = tf.Variable(tf.truncated_normal([2*hidden_size, class_num], stddev=0.1), name='fc_w')\n",
    "    fc_b = tf.Variable(tf.zeros([class_num]), name='fc_b')\n",
    "    #self.final_output = outputs[-1]\n",
    "    # 用于分类任务, outputs取最终一个时刻的输出\n",
    "    logits = tf.matmul(final_output, fc_w) + fc_b\n",
    "    #logits = tf.matmul(outputs[-1], fc_w) + fc_b\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits+1e-10,labels = y)\n",
    "    original_cost_function = tf.reduce_mean(loss)\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = 0.001* tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "    cost = original_cost_function + regularization_cost\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(original_cost_function)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits,1)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: step 1, training accuracy 0.00584057\n",
      "训练集表现： 0.006\n",
      "Iter 1: step 21, training accuracy 0.72502\n",
      "训练集表现： 0.726\n",
      "Iter 1: step 41, training accuracy 0.835043\n",
      "训练集表现： 0.808\n",
      "Iter 2: step 61, training accuracy 0.867088\n",
      "训练集表现： 0.902\n",
      "Iter 2: step 81, training accuracy 0.889976\n",
      "训练集表现： 0.902\n",
      "Iter 3: step 101, training accuracy 0.902762\n",
      "训练集表现： 0.944\n",
      "Iter 3: step 121, training accuracy 0.908445\n",
      "训练集表现： 0.954\n",
      "Iter 3: step 141, training accuracy 0.913181\n",
      "训练集表现： 0.95\n",
      "Iter 4: step 161, training accuracy 0.915233\n",
      "训练集表现： 0.972\n",
      "Iter 4: step 181, training accuracy 0.919968\n",
      "训练集表现： 0.97\n",
      "Iter 5: step 201, training accuracy 0.9206\n",
      "训练集表现： 0.976\n",
      "Iter 5: step 221, training accuracy 0.921389\n",
      "训练集表现： 0.976\n",
      "Iter 5: step 241, training accuracy 0.921231\n",
      "训练集表现： 0.968\n",
      "Iter 6: step 261, training accuracy 0.917916\n",
      "训练集表现： 0.976\n",
      "Iter 6: step 281, training accuracy 0.927545\n",
      "训练集表现： 0.99\n",
      "Iter 7: step 301, training accuracy 0.921863\n",
      "训练集表现： 0.986\n",
      "Iter 7: step 321, training accuracy 0.919968\n",
      "训练集表现： 0.992\n",
      "Iter 7: step 341, training accuracy 0.92423\n",
      "训练集表现： 0.982\n",
      "Iter 8: step 361, training accuracy 0.919968\n",
      "训练集表现： 0.982\n",
      "Iter 8: step 381, training accuracy 0.919495\n",
      "训练集表现： 0.98\n",
      "Iter 9: step 401, training accuracy 0.925335\n",
      "训练集表现： 0.998\n",
      "Iter 9: step 421, training accuracy 0.928492\n",
      "训练集表现： 0.992\n",
      "Iter 9: step 441, training accuracy 0.922494\n",
      "训练集表现： 0.984\n",
      "Iter 10: step 461, training accuracy 0.920442\n",
      "训练集表现： 0.986\n",
      "Iter 10: step 481, training accuracy 0.923915\n",
      "训练集表现： 0.992\n",
      "Iter 11: step 501, training accuracy 0.918863\n",
      "训练集表现： 0.994\n",
      "Iter 11: step 521, training accuracy 0.92281\n",
      "训练集表现： 0.98\n",
      "Iter 11: step 541, training accuracy 0.920126\n",
      "训练集表现： 0.99\n",
      "Iter 12: step 561, training accuracy 0.915864\n",
      "训练集表现： 0.98\n",
      "Iter 12: step 581, training accuracy 0.916338\n",
      "训练集表现： 0.984\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(frame_index['商品描述'].tolist(),y_label,test_size = 0.2)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    listnum = []\n",
    "    Iter = 0\n",
    "    for i in range(600):\n",
    "        _batch_size = 500\n",
    "        if len(listnum)<_batch_size:\n",
    "            listnum = list(range(len(X_train)))\n",
    "            Iter += 1\n",
    "        temp_x =[]\n",
    "        temp_y = []\n",
    "        for _ in range(_batch_size):\n",
    "            j = random.randint(0,len(listnum)-1) #生成一个包括0，len(listnum)-1之间的随机数\n",
    "            temp_x.append(X_train[listnum[j]])\n",
    "            temp_y.append(Y_train[listnum[j]])\n",
    "            del listnum[j]\n",
    "        # temp_x 是一个1000*1200维度的向量\n",
    "        if (i)%20 == 0:\n",
    "            train_accuracy_test = sess.run(accuracy, feed_dict={input_data:X_test, y: Y_test, keep_prob: 1.0, batch_size:len(X_test)})\n",
    "            train_accuracy_train = sess.run(accuracy, feed_dict={input_data:temp_x, y: temp_y, keep_prob: 1.0, batch_size:len(X_test)})\n",
    "            print(\"Iter %d: step %d, training accuracy %g\" % ( Iter,(i+1), train_accuracy_test))\n",
    "            print('训练集表现：',train_accuracy_train)\n",
    "        sess.run(train_op,feed_dict={input_data:temp_x,y:temp_y,keep_prob: 0.3, batch_size: _batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9006187649955802"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frematrix = CountVectorizer().fit_transform(frame['商品描述'])\n",
    "tfidf = TfidfVectorizer().fit_transform(frame['商品描述'])\n",
    "lr = LogisticRegression()\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(tfidf,frame['标签'])\n",
    "lr.fit(X_train,Y_train)\n",
    "pre = lr.predict(X_test)\n",
    "np.mean(Y_test == pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
