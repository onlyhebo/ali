{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\hebo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.764 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "txt = pd.read_excel('数据比较好的.xlsx')\n",
    "Y = txt['标签']\n",
    "jieba.load_userdict(Y)\n",
    "stopwords = [line.strip() for line in open('../input/stopword.txt', 'r', encoding='utf-8').readlines()] \n",
    "list_des = txt['商品描述'].tolist()\n",
    "X = []\n",
    "for i in range(len(list_des)):\n",
    "    des = list_des[i].replace(' ','')\n",
    "    seg_list = jieba.lcut(des)\n",
    "    word_list = [] \n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:\n",
    "            word_list.append(seg)\n",
    "    X.append((' ').join(word_list))\n",
    "txt['商品描述'] = pd.Series(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt)):\n",
    "    length = len(txt.loc[i]['商品描述'].split())\n",
    "    if length >15:\n",
    "        txt.drop(index = i,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量：40387 分类数目：511\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame(columns=['商品描述','标签'])\n",
    "dic = Counter(Y)\n",
    "for i in dic.keys():\n",
    "    data = txt.loc[txt['标签'] == i]\n",
    "    if dic[i] > 200:\n",
    "        frame = pd.concat([frame,data[:200]],axis = 0)\n",
    "        continue\n",
    "    elif dic[i]>=30:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)\n",
    "frame = frame.reset_index(drop=True)\n",
    "c_num = len(Counter(frame['标签']).keys())\n",
    "print('数据量：%d'%frame.shape[0],'分类数目：%d'%len(Counter(frame['标签']).keys()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "frematrix = CountVectorizer().fit_transform(frame['商品描述'])\n",
    "lr = LogisticRegression()\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(frematrix,frame['标签'],test_size = 0.2)\n",
    "lr.fit(X_train,Y_train)\n",
    "y_pre = lr.predict(X_test)\n",
    "np.mean(y_pre == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "word_txt = []\n",
    "for label in Counter(frame['标签']).keys():\n",
    "    same = frame.loc[frame['标签'] == label]\n",
    "    word = []\n",
    "    for i in same['商品描述']:\n",
    "        word += i.split()\n",
    "    word_txt.append(word)\n",
    "from gensim.models import word2vec\n",
    "#sentences = word2vec.Text8Corpus('分词拿去做word2vect.txt')\n",
    "model = word2vec.Word2Vec(word_txt,size = 200,window = 900,min_count = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "word2vec_matrix = []\n",
    "for line in range(frame.shape[0]):\n",
    "    vector = []\n",
    "    num = 0\n",
    "    words = frame['商品描述'][line].split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            temp = model[word]\n",
    "        except:\n",
    "            continue \n",
    "        else:\n",
    "            vector += list(temp)\n",
    "            num += 1\n",
    "    vector = vector + [0]*200*(15 - num)\n",
    "    word2vec_matrix.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(frame['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 200\n",
    "timestep_size = 15 \n",
    "hidden_size = 256\n",
    "layer_num = 2\n",
    "class_num = c_num\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    batch_size = tf.placeholder(tf.int32, [], name='batch_size_input')\n",
    "    _X = tf.placeholder(tf.float32,[None,3000])\n",
    "    y = tf.placeholder(tf.int64,[None])\n",
    "    x = tf.reshape(_X,[-1,timestep_size,input_size])\n",
    "def Bi_LSTM(x,y,c_num):\n",
    "    with tf.name_scope(\"fw\"):\n",
    "        stacked_rnn_fw = []\n",
    "        for _ in range(layer_num):\n",
    "            fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "            stacked_rnn_fw.append(fw_cell)\n",
    "        lstm_fw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_fw), output_keep_prob= keep_prob)\n",
    "        #lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCellltiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "    with tf.name_scope(\"bw\"):\n",
    "        stacked_rnn_bw = []\n",
    "        for _ in range(layer_num):\n",
    "            bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "            stacked_rnn_bw.append(bw_cell)\n",
    "        lstm_bw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_bw), output_keep_prob= keep_prob)\n",
    "        #lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "    with tf.name_scope(\"output\"):\n",
    "        x = tf.unstack(x, 15, axis=1)\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, x, dtype=tf.float32)\n",
    "    with tf.name_scope(\"result\"):\n",
    "        w = tf.Variable(tf.random_uniform([2 * hidden_size, class_num], -1.0, 1.0), name='W')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\n",
    "        logits = tf.nn.xw_plus_b(outputs[-1], w, b)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits+1e-10,labels = y)\n",
    "        original_cost_function = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(original_cost_function)\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        prediction = tf.argmax(logits,1)\n",
    "        correct_prediction = tf.equal(prediction,y)\n",
    "        correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")\n",
    "    return train_op,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op,accuracy = Bi_LSTM(x,y,c_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(word2vec_matrix,y_label,test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: step 1, training accuracy 0.00272345\n",
      "Iter 2: step 51, training accuracy 0.596435\n",
      "Iter 3: step 101, training accuracy 0.714781\n",
      "Iter 5: step 151, training accuracy 0.761079\n",
      "Iter 6: step 201, training accuracy 0.783857\n",
      "Iter 7: step 251, training accuracy 0.794751\n",
      "Iter 9: step 301, training accuracy 0.815301\n",
      "Iter 10: step 351, training accuracy 0.818272\n",
      "Iter 12: step 401, training accuracy 0.825452\n",
      "Iter 13: step 451, training accuracy 0.824214\n",
      "Iter 14: step 501, training accuracy 0.837831\n",
      "Iter 16: step 551, training accuracy 0.837088\n",
      "Iter 17: step 601, training accuracy 0.826442\n",
      "Iter 19: step 651, training accuracy 0.832137\n",
      "Iter 20: step 701, training accuracy 0.836346\n",
      "Iter 21: step 751, training accuracy 0.841297\n",
      "Iter 23: step 801, training accuracy 0.836593\n",
      "Iter 24: step 851, training accuracy 0.834365\n",
      "Iter 26: step 901, training accuracy 0.838079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-897f05911399>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iter %d: step %d, training accuracy %g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mIter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0m_X\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtemp_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtemp_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_batch_size\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    listnum = []\n",
    "    Iter = 0\n",
    "    for i in range(10001):\n",
    "        _batch_size = 1000\n",
    "        if len(listnum)<_batch_size:\n",
    "            listnum = list(range(len(X_train)))\n",
    "            Iter += 1\n",
    "        temp_x =[]\n",
    "        temp_y = []\n",
    "        for _ in range(_batch_size):\n",
    "            j = random.randint(0,len(listnum)-1) #生成一个包括0，len(listnum)-1之间的随机数\n",
    "            temp_x.append(X_train[listnum[j]])\n",
    "            temp_y.append(Y_train[listnum[j]])\n",
    "            del listnum[j]\n",
    "        # temp_x 是一个1000*1200维度的向量\n",
    "        if (i)%50 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={_X:X_test, y: Y_test, keep_prob: 1.0, batch_size:len(X_test)})\n",
    "            print(\"Iter %d: step %d, training accuracy %g\" % ( Iter,(i+1), train_accuracy))\n",
    "        sess.run(train_op,feed_dict={_X:temp_x,y:temp_y,keep_prob: 0.3, batch_size: _batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 5, 6],\n",
       "       [3, 4, 7, 8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "a = tf.Variable([[1,2],[3,4]])\n",
    "b = tf.Variable([[5,6],[7,8]])\n",
    "c = tf.concat([a,b],1)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
