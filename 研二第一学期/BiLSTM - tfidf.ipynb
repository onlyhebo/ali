{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\njieba.load_userdict(Y)\\nstopwords = [line.strip() for line in open('../input/stopword.txt', 'r', encoding='utf-8').readlines()] \\nlist_des = txt['商品描述'].tolist()\\nX = []\\nfor i in range(len(list_des)):\\n    des = list_des[i].replace(' ','')\\n    seg_list = jieba.lcut(des)\\n    word_list = [] \\n    for seg in seg_list:\\n        if seg not in stopwords:\\n            word_list.append(seg)\\n    X.append((' ').join(word_list))\\ntxt['商品描述'] = pd.Series(X) \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "#txt = pd.read_excel('数据比较好的.xlsx')\n",
    "txt = pd.read_excel('guo(new_3w).xlsx').drop('空白字段',axis = 1)\n",
    "Y = txt['标签']\n",
    "'''\n",
    "jieba.load_userdict(Y)\n",
    "stopwords = [line.strip() for line in open('../input/stopword.txt', 'r', encoding='utf-8').readlines()] \n",
    "list_des = txt['商品描述'].tolist()\n",
    "X = []\n",
    "for i in range(len(list_des)):\n",
    "    des = list_des[i].replace(' ','')\n",
    "    seg_list = jieba.lcut(des)\n",
    "    word_list = [] \n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:\n",
    "            word_list.append(seg)\n",
    "    X.append((' ').join(word_list))\n",
    "txt['商品描述'] = pd.Series(X) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt)):\n",
    "    length = len(txt.loc[i]['商品描述'].split())\n",
    "    if length >15:\n",
    "        txt.drop(index = i,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量：21202 分类数目：511\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame(columns=['商品描述','标签'])\n",
    "dic = Counter(Y)\n",
    "for i in dic.keys():\n",
    "    data = txt.loc[txt['标签'] == i]\n",
    "    if dic[i] > 200:\n",
    "        frame = pd.concat([frame,data[:200]],axis = 0)\n",
    "        continue\n",
    "    elif dic[i]>=10:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)\n",
    "frame = frame.reset_index(drop=True)\n",
    "c_num = len(Counter(frame['标签']).keys())\n",
    "print('数据量：%d'%frame.shape[0],'分类数目：%d'%len(Counter(frame['标签']).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(min_df=5)\n",
    "frematrix = count.fit_transform(frame['商品描述'])\n",
    "tfidf = TfidfVectorizer().fit_transform(frame['商品描述'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.851013672795851"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(tfidf,frame['标签'],test_size = 0.1)\n",
    "lr.fit(X_train,Y_train)\n",
    "pre = lr.predict(X_test)\n",
    "np.mean(pre == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_txt = []\n",
    "for label in Counter(frame['标签']).keys():\n",
    "    same = frame.loc[frame['标签'] == label]\n",
    "    word = []\n",
    "    for i in same['商品描述']:\n",
    "        word += i.split()\n",
    "    word_txt.append(word)\n",
    "from gensim.models import word2vec\n",
    "#sentences = word2vec.Text8Corpus('分词拿去做word2vect.txt')\n",
    "model = word2vec.Word2Vec(word_txt,size = 100,window = 2000,min_count = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "word2vec_matrix = []\n",
    "for line in range(frame.shape[0]):\n",
    "    vector = []\n",
    "    num = 0\n",
    "    words = frame['商品描述'][line].split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            temp = model[word]\n",
    "        except:\n",
    "            continue \n",
    "        else:\n",
    "            vector += list(temp)\n",
    "            num += 1\n",
    "    vector = vector + [0]*100*(15 - num)\n",
    "    word2vec_matrix.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "inputs = hstack((csr_matrix(np.array(word2vec_matrix)),frematrix)) #返回来的是coo_matrix ，不能进行索引，切片什么的\n",
    "inputs_x = inputs.tocsr()#转化为 csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(frame['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "timestep_size = 15 \n",
    "hidden_size = 256\n",
    "layer_num = 2\n",
    "class_num = c_num\n",
    "learning_rate = 0.001\n",
    "fre_shape = frematrix.shape[1]\n",
    "add_size = frematrix.shape[1] + np.shape(word2vec_matrix)[1]\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    batch_size = tf.placeholder(tf.int32, [], name='batch_size_input')\n",
    "    _X = tf.placeholder(tf.float32,[None,1500])\n",
    "    fre = tf.placeholder(tf.float32,[None,fre_shape])\n",
    "    y = tf.placeholder(tf.int64,[None])\n",
    "    x = tf.reshape(_X,[-1,timestep_size,input_size])\n",
    "with tf.name_scope(\"fw\"):\n",
    "    stacked_rnn_fw = []\n",
    "    for _ in range(layer_num):\n",
    "        fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        stacked_rnn_fw.append(fw_cell)\n",
    "    lstm_fw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_fw), output_keep_prob= keep_prob)\n",
    "    #lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCellltiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "with tf.name_scope(\"bw\"):\n",
    "    stacked_rnn_bw = []\n",
    "    for _ in range(layer_num):\n",
    "        bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        stacked_rnn_bw.append(bw_cell)\n",
    "    lstm_bw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_bw), output_keep_prob= keep_prob)\n",
    "    #lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "with tf.name_scope(\"output\"):\n",
    "    x = tf.unstack(x, 15, axis=1)\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, x, dtype=tf.float32)\n",
    "with tf.name_scope('concat'):\n",
    "    output = tf.concat([outputs[-1],fre],1)\n",
    "with tf.name_scope(\"result\"):\n",
    "    w = tf.Variable(tf.random_uniform([2 * hidden_size + fre_shape, class_num], -1.0, 1.0), name='W')\n",
    "    b = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\n",
    "    logits = tf.nn.xw_plus_b(output, w, b)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits+1e-10,labels = y)\n",
    "    original_cost_function = tf.reduce_mean(loss)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(original_cost_function)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits,1)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(inputs_x,y_label,test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\scipy\\sparse\\compressed.py:742: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: step 1, training accuracy 0.0018859\n",
      "Iter 3: step 51, training accuracy 0.576615\n",
      "Iter 6: step 101, training accuracy 0.685526\n",
      "Iter 8: step 151, training accuracy 0.746818\n",
      "Iter 11: step 201, training accuracy 0.789722\n",
      "Iter 14: step 251, training accuracy 0.815653\n",
      "Iter 16: step 301, training accuracy 0.835455\n",
      "Iter 19: step 351, training accuracy 0.851014\n",
      "Iter 22: step 401, training accuracy 0.860443\n",
      "Iter 24: step 451, training accuracy 0.868458\n",
      "Iter 27: step 501, training accuracy 0.871287\n",
      "Iter 29: step 551, training accuracy 0.874587\n",
      "Iter 32: step 601, training accuracy 0.880245\n",
      "Iter 35: step 651, training accuracy 0.880717\n",
      "Iter 37: step 701, training accuracy 0.880245\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8ccd765123be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iter %d: step %d, training accuracy %g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mIter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         sess.run(train_op,feed_dict={_X:temp_x[:,:1500].toarray(),fre:temp_x[:,1500:].toarray(),y:temp_y,\n\u001b[1;32m---> 25\u001b[1;33m                                      keep_prob: 0.3, batch_size: _batch_size})\n\u001b[0m",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    listnum = []\n",
    "    Iter = 0\n",
    "    for i in range(10001):\n",
    "        _batch_size = 1000\n",
    "        if len(listnum)<_batch_size:\n",
    "            listnum = list(range(X_train.shape[0]))\n",
    "            Iter += 1\n",
    "        temp_x = csr_matrix((_batch_size,add_size),dtype=float)\n",
    "        temp_y = []\n",
    "        for k in range(_batch_size):\n",
    "            j = random.randint(0,len(listnum)-1) #生成一个包括0，len(listnum)-1之间的随机数\n",
    "            temp_x[k] = X_train[listnum[j]]\n",
    "            #temp_x.append(X_train[listnum[j]])\n",
    "            temp_y.append(Y_train[listnum[j]])\n",
    "            del listnum[j]\n",
    "        # temp_x 是一个1000*1200维度的向量\n",
    "        #print(temp_x)\n",
    "        if (i)%50 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={_X:X_test[:,:1500].toarray(),fre:X_test[:,1500:].toarray(), y: Y_test, \n",
    "                                                           keep_prob: 1.0, batch_size:X_test.shape[0]})\n",
    "            print(\"Iter %d: step %d, training accuracy %g\" % ( Iter,(i+1), train_accuracy))\n",
    "        sess.run(train_op,feed_dict={_X:temp_x[:,:1500].toarray(),fre:temp_x[:,1500:].toarray(),y:temp_y,\n",
    "                                     keep_prob: 0.3, batch_size: _batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "a = tf.Variable([[1,2],[3,4]])\n",
    "b = tf.Variable([[5,6],[7,8]])\n",
    "c = tf.concat([a,b],1)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
