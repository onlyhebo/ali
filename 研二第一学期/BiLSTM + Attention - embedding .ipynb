{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量：46399 类目数：768\n"
     ]
    }
   ],
   "source": [
    "frame = pd.read_csv('20-200.csv')\n",
    "print('数据量：%d'%frame.shape[0],'类目数：%d'%len(Counter(frame['标签'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length = 15,min_frequency = 10)\n",
    "x_text = np.array(list(vocab_processor.fit_transform(frame['商品描述'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(frame['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "\n",
    "input_size = 100\n",
    "timestep_size = 15 \n",
    "hidden_size = 256\n",
    "layer_num = 2\n",
    "class_num = len(Counter(frame['标签']))\n",
    "learning_rate = 0.001\n",
    "vocabulary_size = len(vocab_processor.vocabulary_)\n",
    "embed_dim = 100\n",
    "sequence_length = 15\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    batch_size = tf.placeholder(tf.int32, [], name='batch_size_input')\n",
    "    y = tf.placeholder(tf.int64,[None])\n",
    "    input_data = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_data')  \n",
    "\n",
    "with tf.name_scope(\"fw\"):\n",
    "    stacked_rnn_fw = []\n",
    "    for _ in range(layer_num):\n",
    "        fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        stacked_rnn_fw.append(fw_cell)\n",
    "    lstm_fw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_fw), output_keep_prob= keep_prob)\n",
    "    #lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "\n",
    "with tf.name_scope(\"bw\"):\n",
    "    stacked_rnn_bw = []\n",
    "    for _ in range(layer_num):\n",
    "        bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        stacked_rnn_bw.append(bw_cell)\n",
    "    lstm_bw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_bw), output_keep_prob= keep_prob)\n",
    "    #lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "with tf.name_scope(\"embedding_layer\"):\n",
    "    embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\n",
    "    inputs=tf.nn.embedding_lookup(embedding,input_data)    \n",
    "#input_data (batch_size , sequence_length)\n",
    "#inputs shape : (batch_size , sequence_length , embed_dim)\n",
    "\n",
    "inputs = tf.transpose(inputs, [1,0,2])\n",
    "#inputs shape : (sequence_length, batch_size, embed_dim)\n",
    "\n",
    "inputs = tf.reshape(inputs, [-1, embed_dim])\n",
    "\n",
    "# 转换成list,里面的每个元素是(batch_size, embed_dim)\n",
    "inputs = tf.split(inputs, sequence_length, 0) \n",
    "with tf.name_scope(\"output\"):\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, inputs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_size = 256\n",
    "with tf.name_scope('attention'), tf.variable_scope('attention'):\n",
    "    attention_w = tf.Variable(tf.truncated_normal([2*hidden_size, attention_size], stddev=0.1), name='attention_w')\n",
    "    attention_b = tf.Variable(tf.constant(0.1, shape=[attention_size]), name='attention_b')\n",
    "    u_list = []\n",
    "    for t in range(15):\n",
    "        u_t = tf.tanh(tf.matmul(outputs[t], attention_w) + attention_b) \n",
    "        u_list.append(u_t)\n",
    "    u_w = tf.Variable(tf.truncated_normal([attention_size, 1], stddev=0.1), name='attention_uw')\n",
    "    attn_z = []\n",
    "    for t in range(15):\n",
    "        z_t = tf.matmul(u_list[t], u_w)\n",
    "        attn_z.append(z_t)\n",
    "    # transform to batch_size * sequence_length\n",
    "    attn_zconcat = tf.concat(attn_z, axis=1)\n",
    "    alpha = tf.nn.softmax(attn_zconcat)\n",
    "    # transform to sequence_length * batch_size * 1 , same rank as outputs\n",
    "    alpha_trans = tf.reshape(tf.transpose(alpha, [1,0]), [15, -1, 1])\n",
    "    final_output = tf.reduce_sum(outputs * alpha_trans, 0)\n",
    "with tf.name_scope(\"result\"):\n",
    "    fc_w = tf.Variable(tf.truncated_normal([2*hidden_size, class_num], stddev=0.1), name='fc_w')\n",
    "    fc_b = tf.Variable(tf.zeros([class_num]), name='fc_b')\n",
    "    #self.final_output = outputs[-1]\n",
    "    # 用于分类任务, outputs取最终一个时刻的输出\n",
    "    logits = tf.matmul(final_output, fc_w) + fc_b\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits+1e-10,labels = y)\n",
    "    original_cost_function = tf.reduce_mean(loss)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(original_cost_function)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits,1)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: step 1, training accuracy 0.0012931\n",
      "Iter 3: step 101, training accuracy 0.0948276\n",
      "Iter 5: step 201, training accuracy 0.3125\n",
      "Iter 8: step 301, training accuracy 0.497629\n",
      "Iter 10: step 401, training accuracy 0.622629\n",
      "Iter 13: step 501, training accuracy 0.67306\n",
      "Iter 15: step 601, training accuracy 0.703664\n",
      "Iter 18: step 701, training accuracy 0.721983\n",
      "Iter 20: step 801, training accuracy 0.724138\n",
      "Iter 22: step 901, training accuracy 0.733405\n",
      "Iter 25: step 1001, training accuracy 0.734698\n",
      "Iter 27: step 1101, training accuracy 0.745905\n",
      "Iter 30: step 1201, training accuracy 0.737931\n",
      "Iter 32: step 1301, training accuracy 0.734483\n",
      "Iter 35: step 1401, training accuracy 0.736638\n",
      "Iter 37: step 1501, training accuracy 0.742457\n",
      "Iter 40: step 1601, training accuracy 0.74569\n",
      "Iter 42: step 1701, training accuracy 0.744612\n",
      "Iter 44: step 1801, training accuracy 0.750647\n",
      "Iter 47: step 1901, training accuracy 0.749784\n",
      "Iter 49: step 2001, training accuracy 0.745043\n",
      "Iter 52: step 2101, training accuracy 0.740086\n",
      "Iter 54: step 2201, training accuracy 0.741595\n",
      "Iter 57: step 2301, training accuracy 0.74569\n",
      "Iter 59: step 2401, training accuracy 0.747845\n",
      "Iter 61: step 2501, training accuracy 0.742888\n",
      "Iter 64: step 2601, training accuracy 0.74569\n",
      "Iter 66: step 2701, training accuracy 0.747414\n",
      "Iter 69: step 2801, training accuracy 0.74375\n",
      "Iter 71: step 2901, training accuracy 0.750216\n",
      "Iter 74: step 3001, training accuracy 0.749138\n",
      "Iter 76: step 3101, training accuracy 0.746767\n",
      "Iter 79: step 3201, training accuracy 0.742241\n",
      "Iter 81: step 3301, training accuracy 0.75194\n",
      "Iter 83: step 3401, training accuracy 0.754741\n",
      "Iter 86: step 3501, training accuracy 0.750862\n",
      "Iter 88: step 3601, training accuracy 0.751724\n",
      "Iter 91: step 3701, training accuracy 0.743103\n",
      "Iter 93: step 3801, training accuracy 0.753664\n",
      "Iter 96: step 3901, training accuracy 0.749569\n",
      "Iter 98: step 4001, training accuracy 0.750862\n",
      "Iter 101: step 4101, training accuracy 0.751509\n",
      "Iter 103: step 4201, training accuracy 0.754957\n",
      "Iter 105: step 4301, training accuracy 0.754095\n",
      "Iter 108: step 4401, training accuracy 0.755172\n",
      "Iter 110: step 4501, training accuracy 0.760345\n",
      "Iter 113: step 4601, training accuracy 0.750431\n",
      "Iter 115: step 4701, training accuracy 0.753233\n",
      "Iter 118: step 4801, training accuracy 0.739655\n",
      "Iter 120: step 4901, training accuracy 0.735129\n",
      "Iter 122: step 5001, training accuracy 0.752155\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(x_text,y_label,test_size = 0.1)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    listnum = []\n",
    "    Iter = 0\n",
    "    for i in range(5001):\n",
    "        _batch_size = 1000\n",
    "        if len(listnum)<_batch_size:\n",
    "            listnum = list(range(len(X_train)))\n",
    "            Iter += 1\n",
    "        temp_x =[]\n",
    "        temp_y = []\n",
    "        for _ in range(_batch_size):\n",
    "            j = random.randint(0,len(listnum)-1) #生成一个包括0，len(listnum)-1之间的随机数\n",
    "            temp_x.append(X_train[listnum[j]])\n",
    "            temp_y.append(Y_train[listnum[j]])\n",
    "            del listnum[j]\n",
    "        # temp_x 是一个1000*1200维度的向量\n",
    "        if (i)%100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={input_data:X_test, y: Y_test, keep_prob: 1.0, batch_size:len(X_test)})\n",
    "            print(\"Iter %d: step %d, training accuracy %g\" % ( Iter,(i+1), train_accuracy))\n",
    "        sess.run(train_op,feed_dict={input_data:temp_x,y:temp_y,keep_prob: 0.4, batch_size: _batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
