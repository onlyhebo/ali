{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#txt = pd.read_excel('数据比较好的.xlsx')\n",
    "txt1 = pd.read_excel('guo(new_3w).xlsx').drop('空白字段',axis = 1)\n",
    "txt2 = pd.read_excel('数据比较好的.xlsx')\n",
    "txt = pd.concat([txt1,txt2],axis = 0)\n",
    "txt = txt.reset_index(drop = True)\n",
    "Y = txt['标签']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt)):\n",
    "    length = len(txt.loc[i]['商品描述'].split())\n",
    "    if length >15:\n",
    "        txt.drop(index = i,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量：105403 分类数目：1687\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame(columns=['商品描述','标签'])\n",
    "dic = Counter(Y)\n",
    "for i in dic.keys():\n",
    "    data = txt.loc[txt['标签'] == i]\n",
    "    '''if dic[i] > 200:\n",
    "        frame = pd.concat([frame,data[:200]],axis = 0)\n",
    "        continue\n",
    "    elif dic[i]>=10:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)'''\n",
    "    if dic[i]>=10:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)\n",
    "frame = frame.reset_index(drop=True)\n",
    "c_num = len(Counter(frame['标签']).keys())\n",
    "print('数据量：%d'%frame.shape[0],'分类数目：%d'%len(Counter(frame['标签']).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<105403x13792 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 954706 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(min_df=5)\n",
    "frematrix = count.fit_transform(frame['商品描述'])\n",
    "tfidf = TfidfVectorizer().fit_transform(frame['商品描述'])\n",
    "frematrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "word_txt = []\n",
    "for label in Counter(frame['标签']).keys():\n",
    "    same = frame.loc[frame['标签'] == label]\n",
    "    word = []\n",
    "    for i in same['商品描述']:\n",
    "        word += i.split()\n",
    "    word_txt.append(word)\n",
    "from gensim.models import word2vec\n",
    "#sentences = word2vec.Text8Corpus('分词拿去做word2vect.txt')\n",
    "model = word2vec.Word2Vec(word_txt,size = 200,window = 2000,min_count = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "token_dictionary = {}\n",
    "token_index = 0\n",
    "word2vec_matrix = []\n",
    "for line in range(len(frame['商品描述'])):\n",
    "    tokens = frame['商品描述'][line].split()\n",
    "    del_list = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            temp = model[token]\n",
    "        except:\n",
    "            del_list.append(token)\n",
    "            continue\n",
    "        else:\n",
    "            if token not in token_dictionary:\n",
    "                word2vec_matrix.append(list(temp))\n",
    "                token_dictionary[token] = token_index\n",
    "                token_index += 1\n",
    "    for token in del_list:\n",
    "        tokens.remove(token)\n",
    "    frame['商品描述'][line] = ' '.join(tokens)\n",
    "word2vec_matrix.append([0]*200)\n",
    "token_dictionary['</s>'] = token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_index = pd.DataFrame(columns=['商品描述','标签'])\n",
    "padding_index = len(token_dictionary) - 1\n",
    "index = []\n",
    "for line in frame['商品描述']:\n",
    "    tokens = line.split()\n",
    "    line_index = []\n",
    "    for token in tokens:\n",
    "        line_index += [token_dictionary[token]]\n",
    "    for _ in range(len(line_index),15):\n",
    "        line_index.append(padding_index)\n",
    "    index.append(line_index)\n",
    "frame_index['商品描述'] = pd.Series(index)\n",
    "frame_index['标签'] = frame['标签']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "inputs = hstack((csr_matrix(np.array(list(frame_index['商品描述']))),frematrix)) #返回来的是coo_matrix ，不能进行索引，切片什么的\n",
    "inputs_x = inputs.tocsr()#转化为 csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(frame_index['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "input_size = 200\n",
    "timestep_size = 15\n",
    "hidden_size = 256\n",
    "layer_num = 2\n",
    "class_num = c_num\n",
    "learning_rate = 0.01\n",
    "embed_dim = 200\n",
    "sequence_length = 15\n",
    "with tf.name_scope('inputs'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    batch_size = tf.placeholder(tf.int32, [], name='batch_size_input')\n",
    "    #_X = tf.placeholder(tf.float32,[None,1500])\n",
    "    y = tf.placeholder(tf.int64,[None])\n",
    "    #x = tf.reshape(_X,[-1,timestep_size,input_size])\n",
    "    input_data = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_data') \n",
    "    tfidf_place = tf.placeholder(tf.float32,shape=[None,frematrix.shape[1]],name = 'tfidf')\n",
    "with tf.name_scope(\"embedding_layer\"):\n",
    "    #embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\n",
    "    embedding = tf.Variable(word2vec_matrix,dtype = tf.float32)\n",
    "    inputs=tf.nn.embedding_lookup(embedding,input_data)    \n",
    "    \n",
    "#inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "#input_data (batch_size , sequence_length)\n",
    "#inputs shape : (batch_size , sequence_length , embed_dim)\n",
    "\n",
    "inputs = tf.transpose(inputs, [1,0,2])\n",
    "#inputs shape : (sequence_length, batch_size, embed_dim)\n",
    "\n",
    "inputs = tf.reshape(inputs, [-1, embed_dim])\n",
    "\n",
    "# 转换成list,里面的每个元素是(batch_size, embed_dim)\n",
    "inputs = tf.split(inputs, sequence_length, 0)\n",
    "\n",
    "with tf.name_scope(\"fw\"):\n",
    "    stacked_rnn_fw = []\n",
    "    for _ in range(layer_num):\n",
    "        #fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        fw_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "        stacked_rnn_fw.append(fw_cell)\n",
    "    lstm_fw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_fw), output_keep_prob= keep_prob)\n",
    "    #lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "with tf.name_scope(\"bw\"):\n",
    "    stacked_rnn_bw = []\n",
    "    for _ in range(layer_num):\n",
    "        #bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        bw_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "        stacked_rnn_bw.append(bw_cell)\n",
    "    lstm_bw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_bw), output_keep_prob= keep_prob)\n",
    "    #lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "with tf.name_scope(\"output\"):\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, inputs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_size = 256\n",
    "with tf.name_scope('attention'), tf.variable_scope('attention'):\n",
    "    attention_w = tf.Variable(tf.truncated_normal([2*hidden_size, attention_size], stddev=0.1), name='attention_w')\n",
    "    attention_b = tf.Variable(tf.constant(0.1, shape=[attention_size]), name='attention_b')\n",
    "    u_list = []\n",
    "    for t in range(15):\n",
    "        u_t = tf.tanh(tf.matmul(outputs[t], attention_w) + attention_b) \n",
    "        u_list.append(u_t)\n",
    "    u_w = tf.Variable(tf.truncated_normal([attention_size, 1], stddev=0.1), name='attention_uw')\n",
    "    attn_z = []\n",
    "    for t in range(15):\n",
    "        z_t = tf.matmul(u_list[t], u_w)\n",
    "        attn_z.append(z_t)\n",
    "    # transform to batch_size * sequence_length\n",
    "    attn_zconcat = tf.concat(attn_z, axis=1)\n",
    "    alpha = tf.nn.softmax(attn_zconcat)\n",
    "    # transform to sequence_length * batch_size * 1 , same rank as outputs\n",
    "    alpha_trans = tf.reshape(tf.transpose(alpha, [1,0]), [15, -1, 1])\n",
    "    final_output = tf.reduce_sum(outputs * alpha_trans, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"result\",reuse = tf.AUTO_REUSE):\n",
    "    fc_w = tf.Variable(tf.truncated_normal([2*hidden_size, class_num], stddev=0.1), name='fc_w')\n",
    "    fc_b = tf.Variable(tf.zeros([class_num]), name='fc_b')\n",
    "    #self.final_output = outputs[-1]\n",
    "    # 用于分类任务, outputs取最终一个时刻的输出\n",
    "    logits = tf.matmul(final_output, fc_w) + fc_b\n",
    "    #logits = tf.matmul(outputs[-1], fc_w) + fc_b\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits+1e-10,labels = y)\n",
    "    original_cost_function = tf.reduce_mean(loss)\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = 0.001* tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "    cost = original_cost_function + regularization_cost\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(original_cost_function)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits,1)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: step 1, training accuracy 0.000379489\n",
      "训练集表现： 0.0\n",
      "Iter 1: step 21, training accuracy 0.583654\n",
      "训练集表现： 0.582\n",
      "Iter 1: step 41, training accuracy 0.656705\n",
      "训练集表现： 0.668\n",
      "Iter 1: step 61, training accuracy 0.700346\n",
      "训练集表现： 0.71\n",
      "Iter 1: step 81, training accuracy 0.729614\n",
      "训练集表现： 0.742\n",
      "Iter 1: step 101, training accuracy 0.749205\n",
      "训练集表现： 0.782\n",
      "Iter 1: step 121, training accuracy 0.763816\n",
      "训练集表现： 0.754\n",
      "Iter 1: step 141, training accuracy 0.779897\n",
      "训练集表现： 0.784\n",
      "Iter 1: step 161, training accuracy 0.790427\n",
      "训练集表现： 0.83\n",
      "Iter 2: step 181, training accuracy 0.796736\n",
      "训练集表现： 0.894\n",
      "Iter 2: step 201, training accuracy 0.808074\n",
      "训练集表现： 0.848\n",
      "Iter 2: step 221, training accuracy 0.807836\n",
      "训练集表现： 0.832\n",
      "Iter 2: step 241, training accuracy 0.814098\n",
      "训练集表现： 0.856\n",
      "Iter 2: step 261, training accuracy 0.813481\n",
      "训练集表现： 0.86\n",
      "Iter 2: step 281, training accuracy 0.818178\n",
      "训练集表现： 0.844\n",
      "Iter 2: step 301, training accuracy 0.819126\n",
      "训练集表现： 0.828\n",
      "Iter 2: step 321, training accuracy 0.813908\n",
      "训练集表现： 0.854\n",
      "Iter 3: step 341, training accuracy 0.816565\n",
      "训练集表现： 0.884\n",
      "Iter 3: step 361, training accuracy 0.827096\n",
      "训练集表现： 0.914\n",
      "Iter 3: step 381, training accuracy 0.832171\n",
      "训练集表现： 0.93\n",
      "Iter 3: step 401, training accuracy 0.83293\n",
      "训练集表现： 0.924\n",
      "Iter 3: step 421, training accuracy 0.834211\n",
      "训练集表现： 0.878\n",
      "Iter 3: step 441, training accuracy 0.830321\n",
      "训练集表现： 0.864\n",
      "Iter 3: step 461, training accuracy 0.834258\n",
      "训练集表现： 0.862\n",
      "Iter 3: step 481, training accuracy 0.832219\n",
      "训练集表现： 0.89\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e65b2f25e36a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iter %d: step %d, training accuracy %g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mIter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'训练集表现：'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_accuracy_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtemp_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtemp_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_batch_size\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(frame_index['商品描述'].tolist(),y_label,test_size = 0.2)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    listnum = []\n",
    "    Iter = 0\n",
    "    for i in range(600):\n",
    "        _batch_size = 500\n",
    "        if len(listnum)<_batch_size:\n",
    "            listnum = list(range(len(X_train)))\n",
    "            Iter += 1\n",
    "        temp_x =[]\n",
    "        temp_y = []\n",
    "        for _ in range(_batch_size):\n",
    "            j = random.randint(0,len(listnum)-1) #生成一个包括0，len(listnum)-1之间的随机数\n",
    "            temp_x.append(X_train[listnum[j]])\n",
    "            temp_y.append(Y_train[listnum[j]])\n",
    "            del listnum[j]\n",
    "        # temp_x 是一个1000*1200维度的向量\n",
    "        if (i)%20 == 0:\n",
    "            train_accuracy_test = sess.run(accuracy, feed_dict={input_data:X_test, y: Y_test, keep_prob: 1.0, batch_size:len(X_test)})\n",
    "            train_accuracy_train = sess.run(accuracy, feed_dict={input_data:temp_x, y: temp_y, keep_prob: 1.0, batch_size:len(X_test)})\n",
    "            print(\"Iter %d: step %d, training accuracy %g\" % ( Iter,(i+1), train_accuracy_test))\n",
    "            print('训练集表现：',train_accuracy_train)\n",
    "        sess.run(train_op,feed_dict={input_data:temp_x,y:temp_y,keep_prob: 0.3, batch_size: _batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frematrix = CountVectorizer().fit_transform(frame['商品描述'])\n",
    "tfidf = TfidfVectorizer().fit_transform(frame['商品描述'])\n",
    "lr = LogisticRegression()\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(tfidf,frame['标签'],test_size = 0.2)\n",
    "lr.fit(X_train,Y_train)\n",
    "pre = lr.predict(X_test)\n",
    "np.mean(Y_test == pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
