{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\hebo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.756 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "txt = pd.read_excel('数据比较好的.xlsx')\n",
    "Y = txt['标签']\n",
    "jieba.load_userdict(Y)\n",
    "stopwords = [line.strip() for line in open('../input/stopword.txt', 'r', encoding='utf-8').readlines()] \n",
    "list_des = txt['商品描述'].tolist()\n",
    "X = []\n",
    "for i in range(len(list_des)):\n",
    "    des = list_des[i].replace(' ','')\n",
    "    seg_list = jieba.lcut(des)\n",
    "    word_list = [] \n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:\n",
    "            word_list.append(seg)\n",
    "    X.append((' ').join(word_list))\n",
    "txt['商品描述'] = pd.Series(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt)):\n",
    "    length = len(txt.loc[i]['商品描述'].split())\n",
    "    if length >15:\n",
    "        txt.drop(index = i,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量：40387 分类数目：511\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame(columns=['商品描述','标签'])\n",
    "dic = Counter(Y)\n",
    "for i in dic.keys():\n",
    "    data = txt.loc[txt['标签'] == i]\n",
    "    if dic[i] > 200:\n",
    "        frame = pd.concat([frame,data[:200]],axis = 0)\n",
    "        continue\n",
    "    elif dic[i]>=30:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)\n",
    "frame = frame.reset_index(drop=True)\n",
    "c_num = len(Counter(frame['标签']).keys())\n",
    "print('数据量：%d'%frame.shape[0],'分类数目：%d'%len(Counter(frame['标签']).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "word_txt = []\n",
    "for label in Counter(frame['标签']).keys():\n",
    "    same = frame.loc[frame['标签'] == label]\n",
    "    word = []\n",
    "    for i in same['商品描述']:\n",
    "        word += i.split()\n",
    "    word_txt.append(word)\n",
    "from gensim.models import word2vec\n",
    "#sentences = word2vec.Text8Corpus('分词拿去做word2vect.txt')\n",
    "model = word2vec.Word2Vec(word_txt,size = 100,window = 900,min_count = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "word2vec_matrix = []\n",
    "for line in range(frame.shape[0]):\n",
    "    vector = []\n",
    "    num = 0\n",
    "    words = frame['商品描述'][line].split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            temp = model[word]\n",
    "        except:\n",
    "            continue \n",
    "        else:\n",
    "            vector += list(temp)\n",
    "            num += 1\n",
    "    vector = vector + [0]*100*(15 - num)\n",
    "    word2vec_matrix.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_length = [len(line.replace(' ','')) for line in frame['商品描述']]\n",
    "max(char_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_array = []\n",
    "for line in frame['商品描述']:\n",
    "    char_array.append(' '.join(list(line.replace(' ',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length = 38,min_frequency = 10)\n",
    "char_matrix = np.array(list(vocab_processor.fit_transform(char_array)))\n",
    "char_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(frame['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "\n",
    "word_input_size = 100\n",
    "timestep_size = 15 \n",
    "hidden_size = 256\n",
    "char_hidden_size = 300\n",
    "layer_num = 2\n",
    "class_num = c_num\n",
    "learning_rate = 0.001\n",
    "\n",
    "embed_dim = 100\n",
    "sequence_length = 38\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    output_dropout = tf.placeholder(tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    batch_size = tf.placeholder(tf.int32, [], name='batch_size_input')\n",
    "    _X = tf.placeholder(tf.float32,[None,1500])\n",
    "    y = tf.placeholder(tf.int64,[None])\n",
    "    word_embeddings = tf.reshape(_X,[batch_size,timestep_size,input_size])\n",
    "    char_x = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_data')\n",
    "with tf.variable_scope(\"embedding_layer\",reuse=tf.AUTO_REUSE):\n",
    "    embedding = tf.Variable(tf.random_uniform([char_size, embed_dim], -1.0, 1.0), name='W',dtype=tf.float32)\n",
    "    char_embeddings = tf.nn.embedding_lookup(embedding,char_x)\n",
    "    s = tf.shape(char_embeddings)\n",
    "    char_embeddings=tf.reshape(char_embeddings,[batch_size,sequence_length*embed_dim])\n",
    "    # bi lstm on chars\n",
    "    cell_fw = tf.nn.rnn_cell.BasicLSTMCell(char_hidden_size,state_is_tuple=True)\n",
    "    cell_bw = tf.nn.rnn_cell.BasicLSTMCell(char_hidden_size,state_is_tuple=True)\n",
    "    char_embeddings = tf.reshape(char_embeddings,[-1,38,input_size])\n",
    "    char_embeddings = tf.unstack(char_embeddings, sequence_length, axis=1)\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(cell_fw, cell_bw, char_embeddings, dtype=tf.float32)\n",
    "    \n",
    "    output = tf.reshape(outputs[-1],shape=[batch_size,6, 100])\n",
    "    word_embeddings = tf.concat([word_embeddings, output], 1)\n",
    "with tf.variable_scope(\"bi-lstm\",reuse=tf.AUTO_REUSE):\n",
    "    stacked_rnn_fw = []\n",
    "    for _ in range(layer_num):\n",
    "        fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        stacked_rnn_fw.append(fw_cell)\n",
    "    lstm_fw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_fw), output_keep_prob= keep_prob)\n",
    "    \n",
    "    stacked_rnn_bw = []\n",
    "    for _ in range(layer_num):\n",
    "        bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        stacked_rnn_bw.append(bw_cell)\n",
    "    lstm_bw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_bw), output_keep_prob= keep_prob)\n",
    "    word_embeddings = tf.unstack(word_embeddings, 21, axis=1)\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, word_embeddings, dtype=tf.float32)\n",
    "with tf.variable_scope(\"proj\",reuse=tf.AUTO_REUSE):\n",
    "    w = tf.get_variable(\"W\", dtype=tf.float32,shape=[2*hidden_size, class_num])\n",
    "    b = tf.get_variable(\"b\", shape=[class_num],dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "    logits = tf.nn.xw_plus_b(outputs[-1], w, b)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits+1e-10,labels = y)\n",
    "    original_cost_function = tf.reduce_mean(loss)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(original_cost_function)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits,1)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "attention_size = 256\n",
    "with tf.name_scope('attention'), tf.variable_scope('attention'):\n",
    "    attention_w = tf.Variable(tf.truncated_normal([2*hidden_size, attention_size], stddev=0.1), name='attention_w')\n",
    "    attention_b = tf.Variable(tf.constant(0.1, shape=[attention_size]), name='attention_b')\n",
    "    u_list = []\n",
    "    for t in range(53):\n",
    "        u_t = tf.tanh(tf.matmul(outputs[t], attention_w) + attention_b) \n",
    "        u_list.append(u_t)\n",
    "    u_w = tf.Variable(tf.truncated_normal([attention_size, 1], stddev=0.1), name='attention_uw')\n",
    "    attn_z = []\n",
    "    for t in range(53):\n",
    "        z_t = tf.matmul(u_list[t], u_w)\n",
    "        attn_z.append(z_t)\n",
    "    # transform to batch_size * sequence_length\n",
    "    attn_zconcat = tf.concat(attn_z, axis=1)\n",
    "    alpha = tf.nn.softmax(attn_zconcat)\n",
    "    # transform to sequence_length * batch_size * 1 , same rank as outputs\n",
    "    alpha_trans = tf.reshape(tf.transpose(alpha, [1,0]), [53, -1, 1])\n",
    "    final_output = tf.reduce_sum(outputs * alpha_trans, 0)\n",
    "with tf.variable_scope(\"result\",reuse = tf.AUTO_REUSE):\n",
    "    fc_w = tf.Variable(tf.truncated_normal([2*hidden_size, class_num], stddev=0.1), name='fc_w')\n",
    "    fc_b = tf.Variable(tf.zeros([class_num]), name='fc_b')\n",
    "    #self.final_output = outputs[-1]\n",
    "    # 用于分类任务, outputs取最终一个时刻的输出\n",
    "    logits = tf.matmul(final_output, fc_w) + fc_b\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits+1e-10,labels = y)\n",
    "    original_cost_function = tf.reduce_mean(loss)\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = 0.001* tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "    cost = original_cost_function + regularization_cost\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(original_cost_function)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits,1)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix = np.hstack((np.array(word2vec_matrix),char_matrix))\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_matrix,y_label,test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: step 1, training accuracy 0.00247586\n",
      "Iter 2: step 51, training accuracy 0.501114\n",
      "Iter 3: step 101, training accuracy 0.666502\n",
      "Iter 5: step 151, training accuracy 0.738302\n",
      "Iter 6: step 201, training accuracy 0.772468\n",
      "Iter 7: step 251, training accuracy 0.785095\n",
      "Iter 9: step 301, training accuracy 0.801188\n",
      "Iter 10: step 351, training accuracy 0.804407\n"
     ]
    }
   ],
   "source": [
    "y_pre_list = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    listnum = []\n",
    "    Iter = 0\n",
    "    for i in range(5001):\n",
    "        _batch_size = 1000\n",
    "        if len(listnum)<_batch_size:\n",
    "            listnum = list(range(len(X_train)))\n",
    "            Iter += 1\n",
    "        temp_x =[]\n",
    "        temp_y = []\n",
    "        for _ in range(_batch_size):\n",
    "            j = random.randint(0,len(listnum)-1) #生成一个包括0，len(listnum)-1之间的随机数\n",
    "            temp_x.append(X_train[listnum[j]])\n",
    "            temp_y.append(Y_train[listnum[j]])\n",
    "            del listnum[j]\n",
    "        temp_x = np.array(temp_x)\n",
    "        # temp_x 是一个1000*1200维度的向量\n",
    "        #print(temp_x)\n",
    "        if (i)%50 == 0:\n",
    "            #y_pre_list.append(sess.run(prediction, feed_dict={_X:X_test[:,:1500],char_x:X_test[:,1500:],y: Y_test, keep_prob: 1.0, batch_size:len(X_test)}))\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={_X:X_test[:,:1500],char_x:X_test[:,1500:],y: Y_test, keep_prob: 1.0, batch_size:len(X_test),output_dropout:1})\n",
    "            print(\"Iter %d: step %d, training accuracy %g\" % ( Iter,(i+1), train_accuracy))\n",
    "        sess.run(train_op,feed_dict={_X:temp_x[:,:1500],char_x:temp_x[:,1500:],y: temp_y, keep_prob: 0.3, batch_size:_batch_size,output_dropout:0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "b = tf.Variable([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "s = tf.shape(a)\n",
    "c = tf.concat((a,b),1)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(c)\n",
    "#sess.run(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pre = le.inverse_transform(y_pre_list[-1])\n",
    "y_true = le.inverse_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pre)):\n",
    "    if y_pre[i] != y_true[i]:\n",
    "        print(y_pre[i],y_true[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = np.array([[5,6],[7,8]])\n",
    "np.hstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2],[3,4]]\n",
    "a[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reshape(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1]\n",
    "b = [2]\n",
    "np.concat([a,b],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
