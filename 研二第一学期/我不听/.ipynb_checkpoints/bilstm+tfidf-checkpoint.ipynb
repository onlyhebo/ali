{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#txt = pd.read_excel('数据比较好的.xlsx')\n",
    "txt1 = pd.read_excel('../guo(new_3w).xlsx').drop('空白字段',axis = 1)\n",
    "txt2 = pd.read_excel('../数据比较好的.xlsx')\n",
    "txt = pd.concat([txt1,txt2],axis = 0)\n",
    "txt = txt.drop_duplicates(subset=['商品描述'], keep='first')\n",
    "txt = txt.reset_index(drop = True)\n",
    "Y = txt['标签']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt)):\n",
    "    length = len(txt.loc[i]['商品描述'].split())\n",
    "    if length >15:\n",
    "        txt.drop(index = i,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量：89778 分类数目：954\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame(columns=['商品描述','标签'])\n",
    "dic = Counter(Y)\n",
    "for i in dic.keys():\n",
    "    data = txt.loc[txt['标签'] == i]\n",
    "    \"\"\"if dic[i] > 300:\n",
    "        frame = pd.concat([frame,data[:300]],axis = 0)\n",
    "        continue\n",
    "    elif dic[i]>=30:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)\"\"\"\n",
    "    if dic[i]>= 20:\n",
    "        frame = pd.concat([frame,txt.loc[txt['标签'] == i]],axis = 0)\n",
    "frame = frame.reset_index(drop=True)\n",
    "c_num = len(Counter(frame['标签']).keys())\n",
    "print('数据量：%d'%frame.shape[0],'分类数目：%d'%len(Counter(frame['标签']).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<89778x11912 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 823233 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(min_df=5)\n",
    "frematrix = count.fit_transform(frame['商品描述'])\n",
    "tfidf = TfidfVectorizer().fit_transform(frame['商品描述'])\n",
    "frematrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "word_txt = []\n",
    "for label in Counter(frame['标签']).keys():\n",
    "    same = frame.loc[frame['标签'] == label]\n",
    "    word = []\n",
    "    for i in same['商品描述']:\n",
    "        word += i.split()\n",
    "    word_txt.append(word)\n",
    "from gensim.models import word2vec\n",
    "#sentences = word2vec.Text8Corpus('分词拿去做word2vect.txt')\n",
    "model = word2vec.Word2Vec(word_txt,size = 200,window = 600,min_count = 10)\n",
    "#model = word2vec.Word2Vec(word_txt,sg = 1,size = 200,window = 10,min_count = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "token_dictionary = {}\n",
    "token_index = 0\n",
    "word2vec_matrix = []\n",
    "for line in range(len(frame['商品描述'])):\n",
    "    tokens = frame['商品描述'][line].split()\n",
    "    del_list = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            temp = model[token]\n",
    "        except:\n",
    "            del_list.append(token)\n",
    "            continue\n",
    "        else:\n",
    "            if token not in token_dictionary:\n",
    "                word2vec_matrix.append(list(temp))\n",
    "                token_dictionary[token] = token_index\n",
    "                token_index += 1\n",
    "    for token in del_list:\n",
    "        tokens.remove(token)\n",
    "    frame['商品描述'][line] = ' '.join(tokens)\n",
    "word2vec_matrix.append([0]*200)\n",
    "token_dictionary['</s>'] = token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_index = pd.DataFrame(columns=['商品描述','标签'])\n",
    "padding_index = len(token_dictionary) - 1\n",
    "index = []\n",
    "for line in frame['商品描述']:\n",
    "    tokens = line.split()\n",
    "    line_index = []\n",
    "    for token in tokens:\n",
    "        line_index += [token_dictionary[token]]\n",
    "    for _ in range(len(line_index),15):\n",
    "        line_index.append(padding_index)\n",
    "    index.append(line_index)\n",
    "frame_index['商品描述'] = pd.Series(index)\n",
    "frame_index['标签'] = frame['标签']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "inputs = hstack((csr_matrix(np.array(list(frame_index['商品描述']))),frematrix)) #返回来的是coo_matrix ，不能进行索引，切片什么的\n",
    "inputs_x = inputs.tocsr()#转化为 csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "le = preprocessing.LabelEncoder()\n",
    "y_label = le.fit_transform(frame_index['标签'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "input_size = 200\n",
    "timestep_size = 15\n",
    "hidden_size = 256\n",
    "layer_num = 2\n",
    "class_num = c_num\n",
    "learning_rate = 0.01\n",
    "embed_dim = 200\n",
    "sequence_length = 15\n",
    "with tf.name_scope('inputs'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    batch_size = tf.placeholder(tf.int32, [], name='batch_size_input')\n",
    "    #_X = tf.placeholder(tf.float32,[None,1500])\n",
    "    y = tf.placeholder(tf.int64,[None])\n",
    "    #x = tf.reshape(_X,[-1,timestep_size,input_size])\n",
    "    input_data = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_data') \n",
    "    tfidf_place = tf.placeholder(tf.float32,shape=[None,frematrix.shape[1]],name = 'tfidf')\n",
    "with tf.name_scope(\"embedding_layer\"):\n",
    "    #embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\n",
    "    embedding = tf.Variable(word2vec_matrix,dtype = tf.float32)\n",
    "    inputs=tf.nn.embedding_lookup(embedding,input_data)    \n",
    "    \n",
    "#inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "#input_data (batch_size , sequence_length)\n",
    "#inputs shape : (batch_size , sequence_length , embed_dim)\n",
    "\n",
    "inputs = tf.transpose(inputs, [1,0,2])\n",
    "#inputs shape : (sequence_length, batch_size, embed_dim)\n",
    "\n",
    "inputs = tf.reshape(inputs, [-1, embed_dim])\n",
    "\n",
    "# 转换成list,里面的每个元素是(batch_size, embed_dim)\n",
    "inputs = tf.split(inputs, sequence_length, 0)\n",
    "\n",
    "with tf.name_scope(\"fw\"):\n",
    "    stacked_rnn_fw = []\n",
    "    for _ in range(layer_num):\n",
    "        #fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        fw_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "        stacked_rnn_fw.append(fw_cell)\n",
    "    lstm_fw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_fw), output_keep_prob= keep_prob)\n",
    "    #lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "with tf.name_scope(\"bw\"):\n",
    "    stacked_rnn_bw = []\n",
    "    for _ in range(layer_num):\n",
    "        #bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        bw_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "        stacked_rnn_bw.append(bw_cell)\n",
    "    lstm_bw_cell_m = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(stacked_rnn_bw), output_keep_prob= keep_prob)\n",
    "    #lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "with tf.name_scope(\"output\"):\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, inputs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "attention_size = 256\n",
    "with tf.name_scope('attention'), tf.variable_scope('attention'):\n",
    "    attention_w = tf.Variable(tf.truncated_normal([2*hidden_size, attention_size], stddev=0.1), name='attention_w')\n",
    "    attention_b = tf.Variable(tf.constant(0.1, shape=[attention_size]), name='attention_b')\n",
    "    u_list = []\n",
    "    for t in range(15):\n",
    "        u_t = tf.tanh(tf.matmul(outputs[t], attention_w) + attention_b) \n",
    "        u_list.append(u_t)\n",
    "    u_w = tf.Variable(tf.truncated_normal([attention_size, 1], stddev=0.1), name='attention_uw')\n",
    "    attn_z = []\n",
    "    for t in range(15):\n",
    "        z_t = tf.matmul(u_list[t], u_w)\n",
    "        attn_z.append(z_t)\n",
    "    # transform to batch_size * sequence_length\n",
    "    attn_zconcat = tf.concat(attn_z, axis=1)\n",
    "    alpha = tf.nn.softmax(attn_zconcat)\n",
    "    # transform to sequence_length * batch_size * 1 , same rank as outputs\n",
    "    alpha_trans = tf.reshape(tf.transpose(alpha, [1,0]), [15, -1, 1])\n",
    "    final_output = tf.reduce_sum(outputs * alpha_trans, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('concat'):\n",
    "    last_final_output = tf.concat([outputs[-1],tfidf_place],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"result\",reuse = tf.AUTO_REUSE):\n",
    "    fc_w = tf.Variable(tf.truncated_normal([2*hidden_size + frematrix.shape[1], class_num], stddev=0.1), name='fc_w')\n",
    "    fc_b = tf.Variable(tf.zeros([class_num]), name='fc_b')\n",
    "    #self.final_output = outputs[-1]\n",
    "    # 用于分类任务, outputs取最终一个时刻的输出\n",
    "    logits = tf.matmul(last_final_output, fc_w) + fc_b\n",
    "    #logits = tf.matmul(outputs[-1], fc_w) + fc_b\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits+1e-10,labels = y)\n",
    "    original_cost_function = tf.reduce_mean(loss)\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = 0.001* tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "    cost = original_cost_function + regularization_cost\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(original_cost_function)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    prediction = tf.argmax(logits,1)\n",
    "    correct_prediction = tf.equal(prediction,y)\n",
    "    correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(inputs_x,y_label,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\scipy\\sparse\\compressed.py:742: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: step 1, training accuracy 0.00105814\n",
      "训练集表现： 0.0\n",
      "Iter 1: step 21, training accuracy 0.539151\n",
      "训练集表现： 0.568\n",
      "Iter 1: step 41, training accuracy 0.663789\n",
      "训练集表现： 0.664\n",
      "Iter 1: step 61, training accuracy 0.722321\n",
      "训练集表现： 0.742\n",
      "Iter 1: step 81, training accuracy 0.763979\n",
      "训练集表现： 0.752\n",
      "Iter 1: step 101, training accuracy 0.788873\n",
      "训练集表现： 0.788\n",
      "Iter 1: step 121, training accuracy 0.814268\n",
      "训练集表现： 0.818\n",
      "Iter 1: step 141, training accuracy 0.826632\n",
      "训练集表现： 0.822\n",
      "Iter 2: step 161, training accuracy 0.835264\n",
      "训练集表现： 0.906\n",
      "Iter 2: step 181, training accuracy 0.842504\n",
      "训练集表现： 0.908\n",
      "Iter 2: step 201, training accuracy 0.845233\n",
      "训练集表现： 0.922\n",
      "Iter 2: step 221, training accuracy 0.847906\n",
      "训练集表现： 0.874\n"
     ]
    }
   ],
   "source": [
    "add_size = frematrix.shape[1] + 15\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    listnum = []\n",
    "    Iter = 0\n",
    "    for i in range(600):\n",
    "        _batch_size = 500\n",
    "        if len(listnum)<_batch_size:\n",
    "            listnum = list(range(X_train.shape[0]))\n",
    "            Iter += 1\n",
    "        temp_x = csr_matrix((_batch_size,add_size),dtype=float)\n",
    "        temp_y = []\n",
    "        for k in range(_batch_size):\n",
    "            j = random.randint(0,len(listnum)-1) #生成一个包括0，len(listnum)-1之间的随机数\n",
    "            temp_x[k] = X_train[listnum[j]]\n",
    "            temp_y.append(Y_train[listnum[j]])\n",
    "            del listnum[j]\n",
    "        # temp_x 是一个1000*1200维度的向量\n",
    "        if (i)%20 == 0:\n",
    "            train_accuracy_test = sess.run(accuracy, feed_dict={input_data:X_test[:,:15].toarray(), tfidf_place:X_test[:,15:].toarray(),\n",
    "                                                                y: Y_test, keep_prob: 1.0, batch_size:X_test.shape[0]})\n",
    "            train_accuracy_train = sess.run(accuracy, feed_dict={input_data:temp_x[:,:15].toarray(), tfidf_place:temp_x[:,15:].toarray(),\n",
    "                                                                y: temp_y, keep_prob: 1.0, batch_size:_batch_size})\n",
    "            print(\"Iter %d: step %d, training accuracy %g\" % ( Iter,(i+1), train_accuracy_test))\n",
    "            print('训练集表现：',train_accuracy_train)\n",
    "        sess.run(train_op,feed_dict={input_data:temp_x[:,:15].toarray(),tfidf_place:temp_x[:,15:].toarray(),\n",
    "                                     y:temp_y,keep_prob: 0.3, batch_size: _batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frematrix = CountVectorizer().fit_transform(frame['商品描述'])\n",
    "tfidf = TfidfVectorizer().fit_transform(frame['商品描述'])\n",
    "lr = LogisticRegression()\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(tfidf,frame['标签'],test_size = 0.2)\n",
    "lr.fit(X_train,Y_train)\n",
    "pre = lr.predict(X_test)\n",
    "np.mean(Y_test == pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
