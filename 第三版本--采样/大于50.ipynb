{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\hebo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.830 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from collections import Counter\n",
    "jieba.add_word('caonidaye')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下做了两个操作：\n",
    "    第一：选择那些标签名存在于我们的商品描述之中的条目；\n",
    "    第二：如果满足条件一，重新分词，将label作为自定义字典；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = open('.././input/train_X.txt','r',encoding = 'utf8').readlines()\n",
    "Y_1 = open('.././input/train_Y.txt','r',encoding = 'utf8').readlines()\n",
    "X = []\n",
    "Y = []\n",
    "X_del = []\n",
    "Y_del = []\n",
    "for i in range(len(Y_1)):\n",
    "    X_1[i] = X_1[i].strip('\\n')\n",
    "    Y_1[i] = Y_1[i].strip('\\n')\n",
    "    description = X_1[i].replace(' ','')\n",
    "    if Y_1[i] in description and Y_1[i] != '':\n",
    "        drop_Y = description.replace(Y_1[i],'caonidaye') # 将描述里面的关键词抽取出来\n",
    "        drop_cut = jieba.lcut(drop_Y) #对剩余的商品描述分词\n",
    "        s = ''\n",
    "        for j in range(len(drop_cut)):\n",
    "            if drop_cut[j] == 'caonidaye':\n",
    "                drop_cut[j] = Y_1[i]\n",
    "            s = s + drop_cut[j] + ' '\n",
    "        X.append(s)\n",
    "        Y.append(Y_1[i])\n",
    "    else:\n",
    "        X_del.append(X_1[i])\n",
    "        Y_del.append(Y_1[i])\n",
    "#筛选词不在商品里面的 筛除 116358 - 104252 = 12096 条数据\n",
    "#X_del用来存放这些垃圾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(Counter(Y).keys()) = 18480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {} # 创建规则字典\n",
    "for i in range(len(X)):\n",
    "    replace = False \n",
    "    label_cadi = [] #找出备选标签\n",
    "    for word in X[i].split():\n",
    "        if word in Y:\n",
    "            label_cadi.append(word)\n",
    "    #将备选标添加到label_cadi里面\n",
    "    if Y[i] not in label_dict.keys():\n",
    "        label_dict[Y[i]] = []\n",
    "    for word_1 in label_cadi:\n",
    "        for word_2 in label_cadi:\n",
    "            if word_1 != word_2:\n",
    "                if word_1 in label_dict.keys():\n",
    "                    if word_2 in label_dict[word_1]:\n",
    "                        Y[i] = word_1\n",
    "                        replace = True\n",
    "                        break\n",
    "                #如果规则字典里面有，将此时Y值更新为规则词典\n",
    "        if replace == True:\n",
    "            break\n",
    "        #如果已经代替，跳出循环\n",
    "    if replace == True:\n",
    "        continue\n",
    "    #如果已经代替。继续下一条商品描述\n",
    "    else:\n",
    "        #label_cadi.remove(Y_refresh[i])\n",
    "        #label_dict[Y_refresh[i]].extend(label_cadi)\n",
    "        for word in label_cadi:\n",
    "            if word!= Y[i]:\n",
    "                label_dict[Y[i]].append(word)\n",
    "     #如果没有代替的话，创建该规则字典   \n",
    "#可以考虑加入if Y_refresh.count(Y_refresh[i]) < 10  直接break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### len(Counter(Y).keys()) = 13397，通过构造数据字典，label的种类变为13398个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 下面做的任务就是统计数目在50-500 之间的类目，而且，分词后词数要大于等于5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_dic = Counter(Y)\n",
    "X_new = [] \n",
    "Y_new = []\n",
    "for i in range(len(X)):\n",
    "    if Y_dic[Y[i]] >= 30 and Y_dic[Y[i]] <= 500:\n",
    "        X_new.append(X[i])\n",
    "        Y_new.append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame = pd.concat([pd.Series(X_new).to_frame(),pd.Series(Y_new).to_frame()],axis = 1)\n",
    "frame.columns = ['商品描述','标签']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 将以上处理之后的数据保存到文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('two_X_new.txt','w',encoding = 'utf8') as X_op:\n",
    "    for i in range(len(X_new)):\n",
    "        X_op.write(X_new[i] + '\\n')\n",
    "with open('two_Y_new.txt','w',encoding = 'utf8') as Y_op:\n",
    "    for i in range(len(Y_new)):\n",
    "        Y_op.write(Y_new[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用word2cev分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "sentences = word2vec.Text8Corpus('two_X_new.txt')\n",
    "model = word2vec.Word2Vec(sentences,size = 50,window = 4 ,min_count = 3)\n",
    "model.wv.save_word2vec_format('two.zh.text.vector',binary =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 下面做的是将每条商品描述的word2vec相加求平均，保存到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "output = open('two_word2cev_60dim.txt','w',encoding = 'utf8')\n",
    "misnum = 0\n",
    "dim = 50\n",
    "for line in range(len(X_new)):\n",
    "    sum = np.zeros((1,dim))\n",
    "    words = X_new[line].split()\n",
    "    num = 1\n",
    "    for word in words:\n",
    "        try:\n",
    "            temp = model[word]\n",
    "        except:\n",
    "            misnum +=1\n",
    "        else:\n",
    "            temp = temp.reshapeape((1,dim))\n",
    "            sum += temp\n",
    "            num = num+1\n",
    "    if num > 1:\n",
    "        num -= 1\n",
    "    sum /= num\n",
    "    for j in range(dim):\n",
    "        output.write('%.4f'%(sum[0][j]) + ' ')\n",
    "    output.write('\\n')\n",
    "output.flush() #刷新缓冲区\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 调用我们的X和Y，以0.8和0.2的比例划分训练集和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = open('two_word2cev_60dim.txt','r',encoding = 'utf8')\n",
    "Y = open('two_Y_new.txt','r',encoding = 'utf8')\n",
    "X_lines = X.readlines()\n",
    "Y_lines = Y.readlines()\n",
    "for i in range(len(X_lines)):\n",
    "    X_lines[i] = X_lines[i].strip('\\n')\n",
    "    X_lines[i] = list(map(float,X_lines[i].split()))\n",
    "    Y_lines[i] = Y_lines[i].strip('\\n')\n",
    "label = LabelEncoder()\n",
    "Y_num = label.fit_transform(Y_lines)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_lines,Y_num,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(X_train,Y_train)\n",
    "y_pre = clf.predict(X_test)\n",
    "result = np.mean(y_pre==Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46852160533401715"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train,Y_train)\n",
    "y_pre = clf.predict(X_test)\n",
    "result = np.mean(y_pre == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4862161815617387"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "clf.fit(np.array(X_train),Y_train)\n",
    "y_pre = clf.predict(X_test)\n",
    "result = np.mean(y_pre == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5354532632388768"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38994"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-de2e94891cba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "sum = np.zeros((1,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
