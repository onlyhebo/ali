{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import jieba \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from math import isnan\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('./input/guo_corpus.txt','r',encoding = 'utf8').readlines()\n",
    "labels = open('./input/guo_labels.txt','r',encoding = 'utf8').readlines()\n",
    "corpus = [des.strip() for des in corpus]\n",
    "labels = [des.strip() for des in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = Counter(labels)\n",
    "for i in range(len(labels)):\n",
    "    if dic[labels[i]] > 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8212861415752741"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform(corpus)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(tfidf,labels,test_size = 0.2)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,Y_train)\n",
    "result_tfidf = LR.predict(X_test)\n",
    "np.mean(result_tfidf == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902542372881356"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_matrix = CountVectorizer().fit_transform(corpus)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(fre_matrix,labels,test_size = 0.2)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,Y_train)\n",
    "result_fre = LR.predict(X_test)\n",
    "np.mean(result_fre == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "sentences = word2vec.Text8Corpus('./input/guo_corpus.txt')\n",
    "model = word2vec.Word2Vec(sentences,size = 100,window = 900 ,min_count = 3)\n",
    "\n",
    "X_word2vec = [] # 存放词向量的列表\n",
    "for line in range(len(corpus)):\n",
    "    sum = np.zeros((1,100))\n",
    "    words = corpus[line].split()\n",
    "    num = 1\n",
    "    for word in words:\n",
    "        try:\n",
    "            temp = model[word]\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            temp = temp.reshape((1,100))\n",
    "            sum += temp\n",
    "            num = num+1\n",
    "    if num > 1:\n",
    "        num -= 1\n",
    "    sum /= num\n",
    "    X_word2vec.append(list(sum[0]))\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_word2vec,labels,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8153040877367896"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,Y_train)\n",
    "result_word2vec = LR.predict(X_test)\n",
    "np.mean(result_word2vec == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "corpus_split = [con.split() for con in corpus]\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "model = Word2Vec(size = 100 ,window = 500, min_count = 3)\n",
    "model.build_vocab(corpus_split)\n",
    "model.train(corpus_split,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "\n",
    "X_word2vec = [] # 存放词向量的列表\n",
    "for line in range(len(corpus)):\n",
    "    sum = np.zeros((1,100))\n",
    "    words = corpus[line].split()\n",
    "    num = 1\n",
    "    for word in words:\n",
    "        try:\n",
    "            temp = model[word]\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            temp = temp.reshape((1,100))\n",
    "            sum += temp\n",
    "            num = num+1\n",
    "    if num > 1:\n",
    "        num -= 1\n",
    "    sum /= num\n",
    "    X_word2vec.append(list(sum[0]))\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_word2vec,labels,test_size = 0.2)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,Y_train)\n",
    "result_word2vec = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5431206380857427"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(result_word2vec == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
